{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_QMDIMUHhDH",
        "outputId": "a124e86e-a721-4f8c-e4da-74538615f117"
      },
      "outputs": [],
      "source": [
        "!pip install keybert[spacy]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nhaSpBR76dk",
        "outputId": "1c391589-50df-439f-82ea-b59b05ba0f76"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGpE8QP_H4U3"
      },
      "outputs": [],
      "source": [
        "# https://github.com/MaartenGr/KeyBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ir0ftUkxH1sR"
      },
      "outputs": [],
      "source": [
        "doc1 = \"Hi everybody, has anybody tried Windows Subsystem for Linux? I am pondering wether to get me a Windows machine and how easy is to get some linux up and running there while piggybacking on all of security\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8YqmaC6NHwS"
      },
      "outputs": [],
      "source": [
        "doc2 = \"spaCy POS tagger is usally used on entire sentences. Is there a way to efficiently apply a unigram POS tagging to a single word (or a list of single words)?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrAwjs_qN-Yi"
      },
      "outputs": [],
      "source": [
        "doc3 = \"This should be the accepted answer. See also Charlie's posted timings, demonstrating the itemgetter class to sort 126% faster on average than the equivalent lambda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swpn0751JaAC"
      },
      "outputs": [],
      "source": [
        "from keybert import KeyBERT\n",
        "import spacy\n",
        "import statistics\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "\n",
        "class KeywordExtractor():\n",
        "  def __init__(self):\n",
        "    self.kw_model = KeyBERT()\n",
        "    \n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"tagger\"]\n",
        "    nlp.disable_pipes(*other_pipes)\n",
        "\n",
        "    self.nlp = nlp\n",
        "\n",
        "  def ranker(self, question, answers):\n",
        " \n",
        "    question_keywords = self.extract(question)\n",
        "    words, scores = zip(*question_keywords)\n",
        "    \n",
        "    results = []\n",
        "    for index, answer in enumerate(answers):\n",
        "      akeywords = self.extract(answer, candidates=words)\n",
        "      awords, ascores = zip(*akeywords)\n",
        "      print(index, awords, ascores)\n",
        "      results.append((index, answer, statistics.mean(ascores)))\n",
        "      \n",
        "    results = sorted(results, key=lambda tup: tup[2], reverse=True)\n",
        "    return results\n",
        "\n",
        "  def _keywords(self, text, candidates=None):\n",
        "    return self.kw_model.extract_keywords(text, stop_words='english', keyphrase_ngram_range=(1, 1), candidates=candidates)\n",
        "\n",
        "  def extract(self,text, candidates=None): \n",
        "    \n",
        "    keywords = self._keywords(text, candidates=candidates)\n",
        "\n",
        "    words, scores = zip(*keywords)\n",
        "    \n",
        "    filtered_keywords = []\n",
        "    for index, doc in enumerate(self.nlp.pipe(words)):\n",
        "        if len(doc) > 0 and \"N\" in doc[0].tag_:\n",
        "          filtered_keywords.append(keywords[index])\n",
        "\n",
        "    filtered_keywords =sorted(filtered_keywords, key=lambda tup: tup[1], reverse=True)\n",
        "    return filtered_keywords\n",
        "\n",
        "  def cluster_keywords(self, keywords, n=2):\n",
        "    candidate_embeddings = self.kw_model.model.embed(keywords)\n",
        "    kmeans = KMeans(n_clusters=n, random_state=0).fit(candidate_embeddings)\n",
        "\n",
        "    lookup = defaultdict(list)\n",
        "    topics = dict()\n",
        "    for i, label in enumerate(kmeans.labels_):\n",
        "      lookup[label].append(i)\n",
        "\n",
        "    for label, center in enumerate(kmeans.cluster_centers_):\n",
        "        scores = []\n",
        "        for i in lookup[label]:\n",
        "          similarity_score = cosine_similarity(center.reshape(1, -1), candidate_embeddings[i].reshape(1, -1))\n",
        "          scores.append((keywords[i], similarity_score))\n",
        "    \n",
        "        topic = sorted(scores, key=lambda tup: tup[1], reverse=True)[0][0]\n",
        "        topics[label] =topic\n",
        "\n",
        "    return list(map(lambda x: topics[x], kmeans.labels_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxDNz0sLIJxg"
      },
      "outputs": [],
      "source": [
        "extractor = KeywordExtractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgpgsnq6OBMJ"
      },
      "outputs": [],
      "source": [
        "doc4 = doc4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0hYkPkINhla",
        "outputId": "9dc151a3-06e8-4726-877b-91de0e97a4fe"
      },
      "outputs": [],
      "source": [
        "extractor.extract(doc4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jSTx7Ij-g92",
        "outputId": "db592f56-d421-44cb-af08-57c06fc7e132"
      },
      "outputs": [],
      "source": [
        "extractor.extract(doc1, candidates=[\"house\", \"linux\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN3twnF39hvd",
        "outputId": "37a38124-cc8a-4104-c604-8d31f8e2ae64"
      },
      "outputs": [],
      "source": [
        "results = extractor.ranker(doc1, [doc2, doc3, doc1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8p99EUN_9uDz",
        "outputId": "e2611884-6452-43ab-a326-65e338328643"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KY3ORdcTfSN",
        "outputId": "b8cbd40b-3507-4ec8-a325-68d61d30ab0e"
      },
      "outputs": [],
      "source": [
        "extractor.cluster_keywords([\"house\", \"linux\", \"computer\", \"building\", \"screen\"], n=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "KeyBert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
